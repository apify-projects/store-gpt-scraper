{
    "title": "GPT Contacts Scraper",
    "type": "object",
    "description": "The crawler scrapes contact details from pages via GPT",
    "schemaVersion": 1,
    "properties": {
        "startUrls": {
            "title": "Start URLs",
            "type": "array",
            "description": "A static list of URLs to scrape. <br><br>For details, see <a href='https://apify.com/drobnikj/extended-gpt-scraper#start-urls' target='_blank' rel='noopener'>Start URLs</a> in README.",
            "prefill": [
                { "url": "https://news.ycombinator.com/" }
            ],
            "editor": "requestListSources"
        },
        "includeUrlGlobs": {
            "title": "Include URLs (globs)",
            "type": "array",
            "description": "Glob patterns matching URLs of pages that will be included in crawling. Combine them with the link selector to tell the scraper where to find links. Omitting the glob patterns will cause the scraper to enqueue all links matched by the link selector.",
            "editor": "globs",
            "default": [],
            "prefill": []
        },
        "excludeUrlGlobs": {
            "title": "Exclude URLs (globs)",
            "type": "array",
            "description": "Glob patterns matching URLs of pages that will be excluded from crawling. Note that this affects only links found on pages, but not Start URLs, which are always crawled.",
            "editor": "globs",
            "default": [],
            "prefill": []
        },
        "linkSelector": {
            "title": "Link selector",
            "type": "string",
            "description": "This is a CSS selector that says which links on the page (<code>&lt;a&gt;</code> elements with <code>href</code> attribute) should be followed and added to the request queue. To filter the links added to the queue, use the <b>Pseudo-URLs</b> setting.<br><br>If <b>Link selector</b> is empty, the page links are ignored.<br><br>For details, see <a href='https://apify.com/drobnikj/extended-gpt-scraper#link-selector' target='_blank' rel='noopener'>Link selector</a> in README.",
            "editor": "textfield",
            "prefill": "a[href]"
        },
        "initialCookies": {
            "title": "Initial cookies",
            "type": "array",
            "description": "Cookies that will be pre-set to all pages the scraper opens. This is useful for pages that require login. The value is expected to be a JSON array of objects with `name`, `value`, 'domain' and 'path' properties. For example: `[{\"name\": \"cookieName\", \"value\": \"cookieValue\"}, \"domain\": \".domain.com\", \"path\": \"/\"}]`.\n\nYou can use the [EditThisCookie](https://chrome.google.com/webstore/detail/editthiscookie/fngmhnnpilhplaeedifhccceomclgfbg) browser extension to copy browser cookies in this format, and paste it here.",
            "default": [],
            "prefill": [],
            "editor": "json"
        },
        "openaiApiKey": {
            "title": "OpenAI API key",
            "type": "string",
            "description": "The API key for accessing OpenAI. You can get it from <a href='https://platform.openai.com/account/api-keys' target='_blank' rel='noopener'>OpenAI platform</a>.",
            "editor": "textfield",
            "isSecret": true
        },
        "targetSelector": {
            "title": "Content selector",
            "type": "string",
            "description": "A CSS selector of the HTML element on the page that will be used in the instruction. Instead of a whole page, you can use only part of the page. For example: \"div#content\".",
            "editor": "textfield",
            "prefill": ""
        },
        "removeElementsCssSelector": {
            "title": "Remove HTML elements (CSS selector)",
            "type": "string",
            "description": "A CSS selector matching HTML elements that will be removed from the DOM, before sending it to GPT processing. This is useful to skip irrelevant page content and save on GPT input tokens. \n\nBy default, the Actor removes usually unwanted elements like scripts, styles and inline images. You can disable the removal by setting this value to some non-existent CSS selector like `dummy_keep_everything`.",
            "editor": "textarea",
            "default": "script, style, noscript, path, svg, xlink",
            "prefill": "script, style, noscript, path, svg, xlink"
        },
        "maxCrawlingDepth": {
            "title": "Max crawling depth",
            "type": "integer",
            "description": "This specifies how many links away from the <b>Start URLs</b> the scraper will descend. This value is a safeguard against infinite crawling depths for misconfigured scrapers.<br><br>If set to <code>0</code>, there is no limit.",
            "minimum": 0,
            "default": 0
        },
        "maxPagesPerCrawl": {
            "title": "Max pages per run",
            "type": "integer",
            "description": "Maximum number of pages that the scraper will open. 0 means unlimited.",
            "minimum": 0,
            "default": 10,
            "unit": "pages"
        },
        "skipGptGlobs": {
            "title": "Skip GPT processing for Globs",
            "type": "array",
            "description": "This setting allows you to specify certain page URLs to skip GPT instructions for. Pages matching these glob patterns will only be crawled for links, excluding them from GPT processing. Useful for intermediary pages used for navigation or undesired content.",
            "editor": "globs",
            "default": [],
            "prefill": []
        },
        "proxyConfiguration": {
            "sectionCaption": "Advanced configuration",
            "title": "Proxy configuration",
            "type": "object",
            "description": "This specifies the proxy servers that will be used by the scraper in order to hide its origin.<br><br>For details, see <a href='https://apify.com/drobnikj/extended-gpt-scraper#proxy-configuration' target='_blank' rel='noopener'>Proxy configuration</a> in README.",
            "prefill": { "useApifyProxy": true },
            "default": { "useApifyProxy": false },
            "editor": "proxy"
        },
        "pageFormatInRequest": {
            "title": "Page format in request",
            "type": "string",
            "description": "In what format to send the content extracted from the page to the GPT. Markdown will take less space allowing for larger requests, while HTML may help include some information like attributes that may otherwise be omitted.",
            "enum": ["HTML", "Markdown"],
            "enumTitles": ["HTML", "Markdown"],
            "default": "Markdown"
        },
        "saveSnapshots": {
            "title": "Save debug snapshots",
            "type": "boolean",
            "description": "For each page store its HTML, screenshot and parsed content (markdown/HTML as it was sent to ChatGPT) adding links to these into the output",
            "editor": "checkbox",
            "default": true
        }
    },
    "required": ["startUrls", "openaiApiKey"]
}
